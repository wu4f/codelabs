
<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,user-scalable=yes">
<meta name="theme-color" content="#4F7DC9">
<meta charset="UTF-8">
<title>9: Securing Generative Applications</title>
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
<link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
<link rel="stylesheet" href="https://storage.googleapis.com/claat-public/codelab-elements.css">
<style>
.success{color:#1e8e3e}.error{color:red} </style>
</head>
<body>
<google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
<google-codelab codelab-gaid="" id="G9_SecuringGenerativeApps" title="9: Securing Generative Applications" environment="web" feedback-link="https://docs.google.com/document/d/1bBkgZhOgNcS1SbpxZLhFQo2SFxZcBU9D4PewHF5_6P8">
<google-codelab-step label="Overview" duration="3">
<p>Front-facing LLM applications can open up enterprises to adversarial attacks in unintended ways. In these exercises, we&#39;ll be examining several ways that LLMs can be subverted.</p>
<p>https://portswigger.net/web-security/llm-attacks</p>
<p>R</p>
<p>W</p>
</google-codelab-step>
</google-codelab>
<script src="https://storage.googleapis.com/claat-public/native-shim.js"></script>
<script src="https://storage.googleapis.com/claat-public/custom-elements.min.js"></script>
<script src="https://storage.googleapis.com/claat-public/prettify.js"></script>
<script src="https://storage.googleapis.com/claat-public/codelab-elements.js"></script>
<script src="//support.google.com/inapp/api.js"></script>
</body>
</html>
